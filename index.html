<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><br /></div>
<br>
</td>
<td id="layout-content">
<h1 style="color:steelblue;">Tam Minh Nguyen</h1>
<table class="imgtable"><tr><td>
<img src="tam_profile.jpeg" width="250px" height="290px" />&nbsp;</td>
<p>Email: <a href="mailto:Minh.Tam.Nguyen@rice.edu">Minh.Tam.Nguyen@rice.edu</font></a>, <a href="tam_cv.pdf"><font color=blue size=+0.3> CV</font></a></p>
<p><b>Education</b></p>
Ph.D. student in Electrical and Computer Engineering, Rice University, Houston, Texas, from 2023 <br />
B.S. in International Economics, UEB, Vietnam National University, Ha Noi, 2018 <br />
<!-- <p> 
<a href="Tam_CV.pdf"><font color=blue size=+0.3> CV</font></a>, <a </font></a>
</p> -->
</td></tr></table>
<h2 style="color:steelblue;">Brief Biography</h2>
<hr>
<p> I am currently a Ph.D. student in Electrical and Computer Engineering at Rice University, where I am advised by <a href="https://richb.rice.edu/">Dr. Richard G. Baraniuk</a>. Prior to my time at Rice University, I participated in the FPT AI Residency program, under the mentorship of <a href="https://tannguyen.blogs.rice.edu/">Dr. Tan Nguyen</a> and <a href="https://nhatptnk8912.github.io/">. I hold a B.S. in International Economics from the University of Economics and Business, Vietnam National University, Ha Noi, which I completed in January 2018.
<h2 style="color:steelblue;">Research Interests</h2>
<hr>
<p>My research aims at understanding and advancing self-attention mechanisms in transformers. Adopting a mathematical approach, I prove that self-attention has connections with various established and well-developed techniques, from probabilistic clustering and non-parametric regression to primal-dual optimization and image denoising. These connections reveal the inherent properties and limitations of self-attention while providing principled frameworks to further develop transformers for real-world applications. </p>

<!-- <h3 style="color:steelblue;"> Conference Submissions</h3>
<!-- <ul>
<!-- <li><p><a href="FourierFormer.pdf"><font color=blue size=+0.3> Transformer with Fourier Integral Attentions</font></a>.  -->
<!-- <br /> -->
<!-- Tan M. Nguyen<font color=red size=-0.3><b>*</b></font>, Minh Pham<font color=red size=-0.3><b>*</b></font>, <b>Tam Nguyen</b>, Khai Nguyen, Stanley J. Osher, Nhat Ho. </p> -->
<!-- </li> -->
</ul> -->
<!-- <ul> -->
<!-- <li><p><a href="FiSHformer.pdf"><font color=blue size=+0.3> FiSHFormer: Transformer with a Finite Admixture of Shared Heads</font></a>.  -->
<!-- <br /> -->
<!-- Tan M. Nguyen<font color=red size=-0.3><b>*</b></font>,<b>Tam Nguyen</b><font color=red size=-0.3><b>*</b></font>, Hai Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham, Khuong Nguyen, Nhat Ho<font color=red size=-0.3><b>**</b></font>,  Stanley J. Osher<font color=red size=-0.3><b>**</b></font>. </p> -->
<!-- </li>
<!-- </ul> -->
<ul> -->
<!-- <li><p><a href="FiAK.pdf"<font color=blue size=+0.3> Probabilistic Framework for Pruning Transformers via a Finite Admixture of Keys</font></a>.  -->
<!-- <br /> -->
<!-- Tan M. Nguyen<font color=red size=-0.3><b>*</b></font>,<b>Tam Nguyen</b><font color=red size=-0.3><b>*</b></font>, Long Bui, Hai Do, Dung Le, Hung Tran-The, Khuong Nguyen, Richard G. Baraniuk, Nhat Ho<font color=red size=-0.3><b>**</b></font>,  Stanley J. Osher<font color=red size=-0.3><b>**</b></font>. -->
<!-- </li> -->
</ul> -->

<h3 style="color:steelblue;"> Conference Publications</h3>
<ul>
<li><p><a href="NeuTRENO.pdf"><font color=blue size=+0.3> Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals</font></a>. <i>Conference on Neural Information Processing Systems (NeurIPS), 2023</i>.
<br />
<b>Tam Nguyen</b>, Tan M. Nguyen, Richard G. Baraniuk.</p>
</li>
</ul>
<ul>
<li><p><a href="Primal_Dual_Transformers.pdf"><font color=blue size=+0.3> A Primal-Dual Framework for Transformers and Neural Networks</font></a>. <i>International Conference on Learning Representations (ICLR) (notable-top-25%), 2023</i>.
<br />
Tan M. Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Tam Nguyen</b><font color=red size=-0.3><b>*</b></font>, Nhat Ho, Andrea L. Bertozzi, Richard G. Baraniuk, Stanley J. Osher.</p>
</li>
</ul>
<ul>
<li><p><a href="FourierFormer.pdf"><font color=blue size=+0.3> FourierFormer: Transformer Meets Generalized Fourier Integral Theorem</font></a>. <i>Conference on Neural Information Processing Systems (NeurIPS), 2022.</i>
<br />
Tan M. Nguyen<font color=red size=-0.3><b>*</b></font>, Minh Pham<font color=red size=-0.3><b>*</b></font>, <b>Tam Nguyen</b>, Khai Nguyen, Stanley J. Osher, Nhat Ho. </p>
</li>
</ul>
<ul>
<li><p><a href="FiSHformer.pdf"><font color=blue size=+0.3> Improving Transformer with an Admixture of Attention Heads</font></a>. <i>Conference on Neural Information Processing Systems (NeurIPS), 2022.</i>
<br />
Tan M. Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Tam Nguyen</b><font color=red size=-0.3><b>*</b></font>, Hai Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham, Khuong Nguyen, Nhat Ho<font color=red size=-0.3><b>**</b></font>,  Stanley J. Osher<font color=red size=-0.3><b>**</b></font>. </p>
</li>
</ul>
 <ul>
<li><p><a href="FiAK.pdf"<font color=blue size=+0.3> Probabilistic Framework for Pruning Transformers via a Finite Admixture of Keys</font></a>. <i> International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2023</i>. 
<br />
Tan M. Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Tam Nguyen</b><font color=red size=-0.3><b>*</b></font>, Long Bui<font color=red size=-0.3><b>*</b></font>, Hai Do, Dung Le, Hung Tran-The, Khuong Nguyen, Richard G. Baraniuk, Nhat Ho<font color=red size=-0.3><b>**</b></font>,  Stanley J. Osher<font color=red size=-0.3><b>**</b></font>.
</li>
</ul>
<ul>
<li><p><a href="MGK.pdf"><font color=blue size=+0.3> Improving Transformers with Probabilistic Attention Keys</font></a>. <i> International Conference on Machine Learning (ICML), 2022</i>.
<br />
<b>Tam Nguyen</b><font color=red size=-0.3><b>*</b></font>, Tan M. Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le, Khuong Nguyen, Anh Tran, Richard G. Baraniuk, Nhat Ho<font color=red size=-0.3><b>**</b></font>,  Stanley J. Osher<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>

<div id="footer">
<div id="footer-text">
<font color=red size=-0.3><b>*</b></font>: co-first authors.
<font color=red size=-0.3><b>**</b></font>: co-last authors.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
